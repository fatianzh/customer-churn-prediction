# -*- coding: utf-8 -*-
"""Challenge #2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LAXNa35LxU_eGKz4zJKVY9-kqp38Ue4c

Challenge 2: Farah Nailal Azzah & Fatia Nurzakiah

# Import Data and Data Overview
"""

import pandas as pd
from google.colab import files
filenya = files.upload ()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

df = pd.read_csv("Data Train.csv")
df.head()

df.shape

df['total_charge'] = df['total_day_charge'] + df['total_eve_charge'] + df['total_night_charge'] + df['total_intl_charge']
df

df.info()

df.describe()

df.describe(include='object')

numerical = df.select_dtypes(include='number')
categorical = df.select_dtypes(include='object')
display(numerical.head(3),categorical.head(3))

"""#Data Cleaning"""

def summary_tab(df):
  summary = pd.DataFrame({
      'Kolom' : df.columns,
      'dataTypes' : df.dtypes,
      'null' : df.isna().sum(),
      'null_percentage' : round(df.isna().sum()/len(df)*100,2),
      'unique' : df.nunique(),
      'unique_sample' : [list(df[i].drop_duplicates().sample(2)) for i in df.columns]
  })
  summary['unique%'] = round(summary['unique']/len(df)*100,2)
  return summary.reset_index(drop=True)

summary_tab(df)

"""###Missing Value"""

#1. Missing Value Check
df.isnull().sum()

"""Tidak terdapat missing value

###Duplicate Value
"""

#2. Duplicated Data Check
df.duplicated().sum()

"""Berdasarkan hasil screening, tidak ditemukan data duplikat

###Outliers Check
"""

#3. Outliers Check
def get_len_outliers(df):

    # Menghitung 25th dan 75th percentile
    p75 = df.quantile(0.75)
    p25 = df.quantile(0.25)
    # Menghitung IQR dengan selisih 25th dan 75th percentile
    iqr = p75 - p25

    # "Minimum non-outlier value": 25th percentile - 1.5 * IQR
    min_val = p25 - 1.5*iqr
    # "Maximum non-outlier value": 75th percentile + 1.5 * IQR
    max_val = p75 + 1.5*iqr

    # Apapun dibawah minimum atau diatas maximum, disebut sebagai outlier
    outliers = df[(df < min_val) | (df > max_val)]
    return len(outliers)

descript = df.describe(include='number').T
descript['num_outlier'] = numerical.apply(get_len_outliers).values
descript

def numerical_plot(df,features):
  fig, axs = plt.subplots(len(numerical.columns), 3, figsize=(45,80))
  for i,col in enumerate(features):
    sns.boxplot(df[col],ax=axs[i][0])
    sns.distplot(df[col],ax=axs[i][1])

    x = sns.kdeplot(df[col][(df['churn']=='yes')],ax=axs[i][2],color='Blue',shade=True)
    x = sns.kdeplot(df[col][(df['churn']=='no')],ax=axs[i][2],color='Red',shade=True)

    x.legend(['yes','no'],loc='upper right')

    axs[i][0].set_title("Mean=%.2f\nMedian=%.2f\nSTD=%.2f" % (df[col].mean(),df[col].median(),df[col].std()))
    plt.setp(axs)
    plt.tight_layout()

numerical_plot(df,numerical.columns)

"""Berdasarkan histogram dapat dilihat bahwa ukuran pemusatan data dari setiap variable berbeda-beda dan hampir disetiap variabel terdapat outlier, sehingga tidak dilakukan penghapusan untuk data-data yang terindikasi outlier karena akan berpengaruh terhadap data churn.

###Drop Columns
"""

#Drop kolom yang dirasa kurang perlu
df_1 = df.drop(['total_day_charge','total_night_charge','total_eve_charge','total_intl_charge'],axis=1)
df_1

numerical_1 = df.select_dtypes(include='number')
categorical_1 = df.select_dtypes(include='object')
display(numerical_1.head(3),categorical_1.head(3))

"""#Exploratory Data Analysis

###Statistic Descriptive
"""

df_stats = df_1.describe ().T
df_stats

df_stats['range'] = df_stats ['max'] - df_stats ['min']
df_stats

df_stats['variance'] = df_stats['std']**2
df_stats

df_total_night_calls = df['total_night_calls']

df_total_night_calls.mean()

df_total_night_calls.median()

df_total_night_calls.mode()

df_total_night_calls.std()

df_stats.loc['total_night_calls','25%']

df['total_night_calls'].median()

def get_outliers(df):
  p75 = df.quantile(0.75)
  p25 = df.quantile(0.25)

  iqr = p75-p25

  min_val = p25 - 1.5*iqr
  max_val = p75 + 1.5*iqr

  outliers = df[(df < min_val) | (df > max_val)]

  return outliers

get_outliers(df['total_night_calls'])

sns.boxplot(df['total_night_calls'])

sns.distplot(df['total_night_calls'])

df_1.corr()

plt.figure(figsize=(15,6))
sns.heatmap(df_1.corr(), annot=True, cmap='viridis')

"""###Churn Distribution Chart"""

# Check many customer are labeled churn
print(df_1['churn'].value_counts())
print(df_1['churn'].value_counts()/len(df_1))

plt.figure(figsize = (7,5))
graph = sns.countplot(df_1, x = 'churn')
graph.set_xticklabels(graph.get_xticklabels())
graph.set_title("Count Churn")

for bar in graph.patches:
  graph.annotate(format(bar.get_height(), '.2f'),
                   (bar.get_x() + bar.get_width() / 2,
                    bar.get_height()), ha='center', va='center',
                   size=10, xytext=(0, 5),
                   textcoords='offset points')

#Churn Distribution by Area Code
plt.figure(figsize = (8,5))
ax = sns.countplot(data = df_1, x='area_code', hue = 'churn')
for bar in ax.patches:
  ax.annotate(format(bar.get_height()),
                   (bar.get_x() + bar.get_width() / 2,
                    bar.get_height()), ha='center', va='center',
                   size=10, xytext=(0, 5),
                   textcoords='offset points')

#Churn Distribution by International Plan
plt.figure(figsize = (8,5))
ax = sns.countplot(data = df_1, x='international_plan', hue = 'churn')
for bar in ax.patches:
  ax.annotate(format(bar.get_height()),
                   (bar.get_x() + bar.get_width() / 2,
                    bar.get_height()), ha='center', va='center',
                   size=10, xytext=(0, 5),
                   textcoords='offset points')

#Churn Distribution by Voice Mail Plan
plt.figure(figsize = (8,5))
ax = sns.countplot(data = df_1, x='voice_mail_plan', hue = 'churn')
for bar in ax.patches:
  ax.annotate(format(bar.get_height()),
                   (bar.get_x() + bar.get_width() / 2,
                    bar.get_height()), ha='center', va='center',
                   size=10, xytext=(0, 5),
                   textcoords='offset points')

df_1['number_customer_service_calls_cat'] = pd.cut(df_1['number_customer_service_calls'], bins=[0,2,5,7,9],
      labels=['0-2','3-5','6-7','8-9'])

#Churn Distribution by Number Customer Service Calls
plt.figure(figsize = (8,5))
ax = sns.countplot(data = df_1, x='number_customer_service_calls_cat', hue = 'churn')
for bar in ax.patches:
  ax.annotate(format(bar.get_height()),
                   (bar.get_x() + bar.get_width() / 2,
                    bar.get_height()), ha='center', va='center',
                   size=10, xytext=(0, 5),
                   textcoords='offset points')

df_1 = df_1.drop(['number_customer_service_calls_cat'],axis=1)

"""###Statistic Inferential"""

# Sampling

df_1.shape[0]

N = df.shape[0]
e = 0.05 # 5% margin of error
slovin_sample = N/(1+N * e**2)

slovin_sample

round(slovin_sample,1)

#sampling
df_sample_366 = df_1.sample (n=366)
df_sample_366

df_sample_366['churn'].value_counts()

df_sample_366.groupby("churn").size()

df_sample_366.head()

# Fraction
df_1.sample(frac=0.25)

# Stratified Sampling
df_sample_stratified = df_1.groupby(['international_plan']).apply(lambda x : x.sample (n=183, random_state=42))
df_sample_stratified

df_sample_stratified['international_plan'].value_counts()

# Experiment Design

df_1['churn'] = df_1['churn'].str.replace('yes', '1')
df_1['churn'] = df_1['churn'].str.replace('no', '0')

df_1 = df_1.astype({'churn':'int'})

no_int_plan = df_1[df_1['international_plan']=='no']
int_plan = df_1[df_1['international_plan']=='yes']
print(f"no_int_plan : {no_int_plan.shape}\nint_plan : {int_plan.shape}")

# Churn Calculation

# Jumlah Churn
n_success_no_int_plan = no_int_plan['churn'].sum()
n_success_int_plan = int_plan['churn'].sum()

# Jumlah Sample
n_obs_no_int_plan = no_int_plan['churn'].count()
n_obs_int_plan = int_plan['churn'].count()

# Churn Rate

# no (international plan)
churn_rate_no_int_plan = n_success_no_int_plan/n_obs_no_int_plan

# yes (international_plan)
churn_rate_int_plan = n_success_int_plan/n_obs_int_plan

print('Conversion Rate No International Plan : {0:0.4f}%'.format(churn_rate_no_int_plan*100))
print('Conversion Rate International Plan : {0:0.4f}%'.format(churn_rate_int_plan*100))

x = ['int_plan','no_int_plan']
value = [churn_rate_int_plan, churn_rate_no_int_plan]

plt.subplots(figsize=(6,4))
plt.bar(x,value)
plt.ylim(0.01,0.50)

# Hypothesis

import statsmodels.stats.proportion as sp

'H0: International Plan tidak berpengaruh terhadap potensi churn'
'H1: International Plan berpengaruh terhadap potensi churn'

# Mengubah menjadi array
success = np.array([n_success_no_int_plan,n_success_int_plan])
obs = np.array([n_obs_no_int_plan,n_obs_int_plan])

stats, pvalue = sp.proportions_ztest(success,obs)

print('P-Value : {0:0.2f}'.format(pvalue))
if pvalue >= 0.05:
    print('Failed to reject H0')
else :
    print('Reject H0')

"""#Data Pre-Processing"""

df_test = pd.read_csv("Data Test.csv")
df_test['total_charge'] = df_test['total_day_charge'] + df_test['total_eve_charge'] + df_test['total_night_charge'] + df_test['total_intl_charge']
df_test

summary_tab(df_test)

"""###Data Cleaning"""

df_test.isnull().sum()

"""Tidak terdapat missing value"""

df_test.duplicated().sum()

"""Berdasarkan hasil screening, tidak ditemukan data duplikat


"""

df_2 = df_1.drop(['state','area_code'],axis=1)

df_test_1 = df_test.drop(['id','total_day_charge','total_night_charge','total_eve_charge','total_intl_charge','state','area_code'],axis=1)
df_test_1

numerical_test = df_test_1.select_dtypes(include='number')
categorical_test = df_test_1.select_dtypes(include='object')
display(numerical_test.head(3),categorical_test.head(3))

"""###One Hot Encoding"""

#1. Converting boolean values to binary

for col in df_2.columns:
    if(df_2[col].astype(str).str.contains('yes').sum()>0 and df_2[col].astype(str).str.contains('no').sum()>0):
        df_2.loc[df_2[col].astype(str).str.contains('yes'), col] = 1
        df_2.loc[df_2[col].astype(str).str.contains('no'), col] = 0

df_2.head()

for col in df_test_1.columns:
    if(df_test_1[col].astype(str).str.contains('yes').sum()>0 and df_test_1[col].astype(str).str.contains('no').sum()>0):
        df_test_1.loc[df_test_1[col].astype(str).str.contains('yes'), col] = 1
        df_test_1.loc[df_test_1[col].astype(str).str.contains('no'), col] = 0

df_test_1.head()

df_test_1.info()

#2. Converting data type to int
df_2 = df_2.astype({"international_plan":'int', "voice_mail_plan":'int'})

df_2.info()

df_test_1 = df_test_1.astype({"international_plan":'int', "voice_mail_plan":'int'})
df_test_1.info()

"""### Correlation Check"""

#1. Chart correlation churn with other variabel
dataC = abs(df_2.corr())['churn'].sort_values().reset_index()
dataC = dataC[~dataC['index'].str.contains('churn')]

plt.figure(figsize=(10,8))
sns.barplot(data = dataC.reset_index(), y='index', x = 'churn')

print("Correlation Values")

corr = abs(df_2.corr()).drop('churn')
corr.sort_values(['churn'], ascending = False, inplace = True)
print(corr.churn)

"""###Train-Test Split"""

from sklearn.model_selection import train_test_split

x = df_2.drop(['churn'], axis=1)
y = df_2['churn']
feature = pd.DataFrame({'feature' : x.columns})

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1764,random_state=42)

print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

"""# Machine Learning Model: Customer Churn Prediction



"""

from sklearn.metrics import roc_auc_score,f1_score,precision_score,accuracy_score,recall_score
from sklearn.metrics import confusion_matrix,classification_report

"""###Logistic Regression"""

# Fit a logistic regression model to our data
from sklearn.linear_model import LogisticRegression

# define models
model1 = LogisticRegression()
model1.fit(x_train,y_train)

# Obtain model predictions
y1_pred = model1.predict(x_test)

print('Classification report:\n', classification_report(y_test, y1_pred))

cm = confusion_matrix(y_test, model1.predict(x_test))
cm = pd.DataFrame(cm , index = ['0', '1'] , columns = ['0', '1'])
plt.figure(figsize = (5,5))
plt.title("Logistic Regression Confusion Matrix")
sns.heatmap(cm, cmap='Blues', annot = True, fmt='',xticklabels = ["1", "0"], yticklabels = ["1", "0"])
plt.xlabel('Predicted')
plt.ylabel('Actual')

accuracy = accuracy_score(y_test,y1_pred)
precision = precision_score(y_test,y1_pred)
f1_s = f1_score(y_test,y1_pred)
roc_auc_sc = roc_auc_score(y_test,y1_pred)

print(f'Test Accuracy Score is:{accuracy}')
print(f'Test Precision Score is: {precision}')
print(f'f1 Score is: {f1_s}')
print(f'ROC AUC Score is: {roc_auc_sc}')

importance_lg = abs(model1.coef_[0])
feature['score'] = importance_lg
feature
# plot feature importance
sns.barplot(data = feature, y = 'feature', x = 'score', color='steelblue')

"""Dengan menggunakan model Logistic Regression, feature yang paling berpengaruh
terhadap customer churn adalah number customer service calls
"""

df_test_1['churn'] = y1_pred

df_test_1

"""###Decision Tree"""

# Define the model as the Decision Tree
from sklearn.tree import DecisionTreeClassifier

# define models
model2 = DecisionTreeClassifier()
model2.fit(x_train, y_train)

# Obtain model predictions
y2_pred = model2.predict(x_test)

print('\nClassification Report:')
print(classification_report(y_test, y2_pred))

cm = confusion_matrix(y_test, model2.predict(x_test))
cm = pd.DataFrame(cm , index = ['0', '1'] , columns = ['0', '1'])
plt.figure(figsize = (5,5))
plt.title("Decision Tree Confusion Matrix")
sns.heatmap(cm, cmap='Blues', annot = True, fmt='',xticklabels = ["0", "1"], yticklabels = ["0", "1"])
plt.xlabel('Predicted')
plt.ylabel('Actual')

accuracy = accuracy_score(y_test,y2_pred)
precision = precision_score(y_test,y2_pred)
f1_s = f1_score(y_test,y2_pred)
roc_auc_sc = roc_auc_score(y_test,y2_pred)

print(f'Test Accuracy Score is:{accuracy}')
print(f'Test Precision Score is: {precision}')
print(f'f1 Score is: {f1_s}')
print(f'ROC AUC Score is: {roc_auc_sc}')

importances_dt = model2.feature_importances_

indices_dt = np.argsort(importances_dt)

fig, ax = plt.subplots()
ax.barh(range(len(importances_dt)), importances_dt[indices_dt], color='steelblue')
ax.set_yticks(range(len(importances_dt)))
_ = ax.set_yticklabels(np.array(x_train.columns)[indices_dt])

"""Dengan menggunakan model Decision Tree, feature yang paling berpengaruh terhadap customer churn adalah total charge"""

df_test_1['churn'] = y2_pred
df_test_1

"""###KNN"""

# Define the model as KNN
from sklearn.neighbors import KNeighborsClassifier

# define models
model3 = KNeighborsClassifier()
model3.fit(x_train, y_train)

# Obtain model predictions
y3_pred = model3.predict(x_test)

print('\nClassification Report:')
print(classification_report(y_test, y3_pred))

cm = confusion_matrix(y_test, model3.predict(x_test))
cm = pd.DataFrame(cm , index = ['0', '1'] , columns = ['0', '1'])
plt.figure(figsize = (5,5))
plt.title("KNN Confusion Matrix")
sns.heatmap(cm, cmap='Blues', annot = True, fmt='',xticklabels = ["0", "1"], yticklabels = ["0", "1"])
plt.xlabel('Predicted')
plt.ylabel('Actual')

accuracy = accuracy_score(y_test,y3_pred)
precision = precision_score(y_test,y3_pred)
f1_s = f1_score(y_test,y3_pred)
roc_auc_sc = roc_auc_score(y_test,y3_pred)

print(f'Test Accuracy Score is:{accuracy}')
print(f'Test Precision Score is: {precision}')
print(f'f1 Score is: {f1_s}')
print(f'ROC AUC Score is: {roc_auc_sc}')

df_test_1['churn'] = y3_pred
df_test_1.head()

"""###Random Forest"""

# Define the model as Random Forest
from sklearn.ensemble import RandomForestClassifier

# Fit the model to our training set
model4 = RandomForestClassifier(8)
model4.fit(x_train,y_train)

# Obtain model predictions
y4_pred = model4.predict(x_test)

print('\nClassification Report:')
print(classification_report(y_test, y4_pred))

cm = confusion_matrix(y_test, model4.predict(x_test))
cm = pd.DataFrame(cm , index = ['0', '1'] , columns = ['0', '1'])
plt.figure(figsize = (5,5))
plt.title("Random Forest Confusion Matrix")
sns.heatmap(cm, cmap='Blues', annot = True, fmt='',xticklabels = ["0", "1"], yticklabels = ["0", "1"])
plt.xlabel('Predicted')
plt.ylabel('Actual')

accuracy = accuracy_score(y_test,y4_pred)
precision = precision_score(y_test,y4_pred)
f1_s = f1_score(y_test,y4_pred)
roc_auc_sc = roc_auc_score(y_test,y4_pred)

print(f'Test Accuracy Score is:{accuracy}')
print(f'Test Precision Score is: {precision}')
print(f'f1 Score is: {f1_s}')
print(f'ROC AUC Score is: {roc_auc_sc}')

importances_rf = model4.feature_importances_

indices_rf = np.argsort(importances_rf)

fig, ax = plt.subplots()
ax.barh(range(len(importances_rf)), importances_rf[indices_rf], color='steelblue')
ax.set_yticks(range(len(importances_rf)))
_ = ax.set_yticklabels(np.array(x_train.columns)[indices_rf])

"""Dengan menggunakan model Random Forest, feature yang paling berpengaruh terhadap customer churn adalah total charge"""

df_test_1['churn'] = y4_pred
df_test_1.head()

"""###Compare Accuracy Model"""

logreg_acc = accuracy_score(y_test, model1.predict(x_test))
logreg_recall = recall_score(y_test, model1.predict(x_test))
logreg_precision = precision_score(y_test, model1.predict(x_test))
logreg_f1 = f1_score(y_test, model1.predict(x_test))
logreg_roc_auc = roc_auc_score(y_test, model1.predict(x_test))

dt_acc = accuracy_score(y_test, model2.predict(x_test))
dt_recall = recall_score(y_test, model2.predict(x_test))
dt_precision = precision_score(y_test, model2.predict(x_test))
dt_f1 = f1_score(y_test, model2.predict(x_test))
dt_roc_auc = roc_auc_score(y_test, model2.predict(x_test))

knn_acc = accuracy_score(y_test, model3.predict(x_test))
knn_recall = recall_score(y_test, model3.predict(x_test))
knn_precision = precision_score(y_test, model3.predict(x_test))
knn_f1 = f1_score(y_test, model3.predict(x_test))
knn_roc_auc = roc_auc_score(y_test, model3.predict(x_test))

rf_acc = accuracy_score(y_test, model4.predict(x_test))
rf_recall = recall_score(y_test, model4.predict(x_test))
rf_precision = precision_score(y_test, model4.predict(x_test))
rf_f1 = f1_score(y_test, model4.predict(x_test))
rf_roc_auc = roc_auc_score(y_test, model4.predict(x_test))

model_performances = pd.DataFrame({
    "Model": ["Logistic Regression", "Decision Tree", "KNN","Random Forest", ],
    "Accuracy": [logreg_acc, dt_acc, knn_acc, rf_acc],
    "Recall": [logreg_recall, dt_recall, knn_recall, rf_recall],
    "Precision": [logreg_precision, dt_precision, knn_precision, rf_precision],
    "F1-Score": [logreg_f1, dt_f1, knn_f1, rf_f1],
    "AUC Score": [logreg_roc_auc, dt_roc_auc, knn_roc_auc, rf_roc_auc]
})

model_performances

"""Model Random Forest memiliki nilai akurasi tertinggi, akan tetapi memiliki selisih jumlah data False Positif dan False Negatif yang sedikit lebih besar dibandingkan dengan model Decision Tree. Sehingga, dapat disimpulkan bahwa model terbaik untuk memprediksi customer churn adalah Decision Tree."""
